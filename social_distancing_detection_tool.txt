import yaml
import imutils
import itertools
import math

import numpy as np 
import cv2

def compute_perspective_transform(corner_points,width,height,image):

  #creating an array from the given corner_points
  corner_points_array=np.float32(corner_points)

  # Creating an array with the parameters (the dimensions) required to build the matrix
  img_params=np.float32([[0,0],[width,0],[0,height],[width,height]])

  # Computing and returning the transformation matrix
  matrix=cv2.getPerspectiveTransform(corner_points_array,img_params) 
  transformed_img=cv2.warpPerspective(image,matrix,(width,height))
  return matrix,transformed_img


def compute_point_perspective_transform(matrix,list_downoids):

  #applying perspective transform on every ground point detected on the main frame
  #list_groundpoints:list that contains the points of transform
  #computing the new coordinates of ground points

  list_points_to_detect=np.float32(list_downoids).reshape(-1, 1, 2)
  transformed_points=cv2.perspectiveTransform(list_points_to_detect, matrix)

  # Loop over the points and add them to the list that will be returned
  transformed_points_list=[]
  for i in range(0,transformed_points.shape[0]):
    transformed_points_list.append([transformed_points[i][0][0],transformed_points[i][0][1]])
  return transformed_points_list

import cv2
import numpy as np
import yaml
import imutils

 
# callback function to get cornor_points from image

def CallBackFunc(event,x,y,flags,param):

    if event==cv2.EVENT_LBUTTONDOWN:
        print("Left button is clicked - position (", x, ", ",y, ")")
        list_points.append([x,y])
    elif event==cv2.EVENT_RBUTTONDOWN:
        print("Right button is clicked - position (", x, ", ", y, ")")
        list_points.append([x,y])


video_cap = cv2.VideoCapture('sample.mp4')
while True:    
    # Load the frame and test if it has reache the end of the video
    ret,frame=video_cap.read()
    cv2.imwrite("frames/static_frame_from_video.jpg",frame)
    break

# Creating a window to contain the image
windowName='MouseCallback'
cv2.namedWindow(windowName)

img_path = "frames/static_frame_from_video.jpg"
img = cv2.imread(img_path)

# calculating the size of the image for the calibration
width,height,channels = img.shape

# an empty list of points for the coordinates of the image
list_points = []

# binding the callback function to the window we created above
cv2.setMouseCallback(windowName,CallBackFunc)

if __name__ == "__main__":
    # Check if the 4 points have been saved
    while True:
        cv2.imshow(windowName,img)
        if len(list_points)==4:
            # Return a dict to the YAML file
            data = dict(
                image_parameters = dict(
                    p2=list_points[3],
                    p1=list_points[2],
                    p4=list_points[0],
                    p3=list_points[1],
                    width_og=width,
                    height_og=height,
                    img_path=img_path,
                    frame_size=800,
                    ))
            # Write the result to the yaml_docs file
            with open('yaml_docs/b.yml','w') as outfile:
                yaml.dump(data, outfile, default_flow_style=False)
            break
        if cv2.waitKey(20) == 27:
            break
    cv2.destroyAllWindows()

!pip install pyyaml==5.1 pycocotools>=2.0.1
import torch, torchvision
print(torch.__version__, torch.cuda.is_available())
!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html

pip install 'git+https://github.com/facebookresearch/detectron2.git'

import detectron2
from detectron2.utils.logger import setup_logger
import numpy as np
import cv2
import random
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog

cfg = get_cfg()

# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_C4_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9  # set threshold for this model

# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_C4_3x.yaml")
predictor = DefaultPredictor(cfg)

def detecting_human_box(boxes,scores,classes):
  idx=np.where(classes==0)[0]
  person=boxes[idx]
  return person

def predict(img):
  out=predictor(img)
  classes=out['instances'].pred_classes.cpu().numpy()
  scores=out['instances'].scores.cpu().numpy()
  boxes=out['instances'].pred_boxes.tensor.cpu().numpy()
  return (boxes,scores,classes)

def getting_groundpoints_and_centroids(array_boxes_detected):
  # Initialize empty centroid and ground point lists
  array_centroids,array_groundpoints=[],[]  
  for index,box in enumerate(array_boxes_detected):
    centroid,ground_point=get_points_from_box(box)
    array_centroids.append(centroid)
    array_groundpoints.append(ground_point)
  return array_centroids,array_groundpoints

def get_points_from_box(box):
  x_center=int(((box[0]+box[2])/2))
  y_center = int(((box[1]+box[3])/2))
  # Coordiniate on the point at the bottom center of the box
  center_y_ground=y_center+((box[3]-box[1])/2)
  return (x_center,y_center),(x_center,int(center_y_ground))

def change_color_on_topview(pair):
	"""
	Draw red circles for the designated pair of points 
	"""
	cv2.circle(bird_view_img,(pair[0][0],pair[0][1]),60,(0,0,255),2)
	cv2.circle(bird_view_img,(pair[0][0],pair[0][1]),3,(0,0,255),-1)
	cv2.circle(bird_view_img,(pair[1][0],pair[1][1]),60,(0,0,255),2)
	cv2.circle(bird_view_img,(pair[1][0],pair[1][1]),3,(0,0,255),-1)

with open("birdview.yml","r") as ymlfile:
    cfg=yaml.load(ymlfile)
width_og,height_og=0,0
corner_points=[]
for section in cfg:
	corner_points.append(cfg["image_parameters"]["p1"])
	corner_points.append(cfg["image_parameters"]["p2"])
	corner_points.append(cfg["image_parameters"]["p3"])
	corner_points.append(cfg["image_parameters"]["p4"])
	width_og=int(cfg["image_parameters"]["width_og"])
	height_og=int(cfg["image_parameters"]["height_og"])
	img_path = cfg["image_parameters"]["img_path"]
	size_frame = 800

minimum_dist='110'

# Compute  transformation matrix from the original frame
matrix,out_img=compute_perspective_transform(corner_points,width_og,height_og,cv2.imread(img_path))
height,width,_=out_img.shape
blank_image=np.zeros((height,width,3), np.uint8)
height=blank_image.shape[0]
width=blank_image.shape[1] 
dim=(width,height)

img=cv2.imread('static_frame_from_video.jpg')
frame=imutils.resize(img,width=int(size_frame))
(boxes,scores,classes)=predict(frame)
array_boxes_detected=detecting_human_box(boxes,scores,classes)
array_centroids,array_groundpoints=getting_groundpoints_and_centroids(array_boxes_detected)
transformed_points_list=compute_point_perspective_transform(matrix,array_groundpoints)

for x1,y1,x2,y2 in array_boxes_detected:
  _ = cv2.rectangle(frame,(x1, y1),(x2, y2),(255,0,0),2)
cv2_imshow(frame)

img2=cv2.imread('chemin_1.png')
bird_view_img=cv2.resize(img2,dim,interpolation=cv2.INTER_AREA)
for point in transformed_points_list:
  x,y = point
  cv2.circle(bird_view_img, (x,y),60,(0,255,0),2)
  cv2.circle(bird_view_img, (x,y),3,(0,255,0),-1)
cv2_imshow(bird_view_img)

capt_video=cv2.VideoCapture('video.avi')
output_video_1,output_video_2 = None,None
# Loop until the end of the video stream

cnt=0
while True:	
  # Load the image of the ground and resize it to the correct size
  img=cv2.imread("chemin_1.png")
  bird_view_img=cv2.resize(img,dim,interpolation=cv2.INTER_AREA)
  cnt+=1
	
	# Load the frame
  (ret, frame)=capt_video.read()
	# Test if it has reached the end of the video
  if not ret:
    print('Problem in reading video')
    break
  else:
    # Resize the image to the correct size
    frame=imutils.resize(frame,width=int(size_frame))
    # Make the predictions for this frame
    (boxes,scores,classes)=predict(frame)
    # Get the human detected in the frame and return the 2 points to build the bounding box  
    array_boxes_detected=detecting_human_box(boxes,scores,classes)
    # Both of our lists that will contain the centroïds coordonates and the ground points
    array_centroids,array_groundpoints=getting_groundpoints_and_centroids(array_boxes_detected)
    # Use the transform matrix to get the transformed coordonates
    transformed_points_list=compute_point_perspective_transform(matrix,array_groundpoints)
    
    # Show every point on the top view image 
    for point in transformed_points_list:
      x,y = point
      cv2.circle(bird_view_img, (x,y),60,(0,255,0),2)
      cv2.circle(bird_view_img, (x,y),3,(0,255,0),-1)

      # Check if 2 or more people have been detected (otherwise no need to detect)
      if len(transformed_points_list) >= 2:
        for index in range(len(transformed_points_list)):
          cv2.rectangle(frame,(array_boxes_detected[index][0],array_boxes_detected[index][1]),(array_boxes_detected[index][2],array_boxes_detected[index][3]),(0,255,0),2)
          # Iterate over every possible 2 by 2 between the points combinations 
          list_indexes = list(itertools.combinations(range(len(transformed_points_list)), 2))
          for i,pair in enumerate(itertools.combinations(transformed_points_list, r=2)):
            # Check if the distance between each combination of points is less than the minimum distance chosen
            if math.sqrt( (pair[0][0] - pair[1][0])**2 + (pair[0][1] - pair[1][1])**2 ) < int(minimum_dist):
              # Change the colors of the points that are too close from each other to red
              change_color_on_topview(pair)
						  # Get the equivalent indexes of these points in the original frame and change the color to red\
              index_pt1 = list_indexes[i][0]
              index_pt2 = list_indexes[i][1]
              cv2.rectangle(frame,(array_boxes_detected[index_pt1][0],array_boxes_detected[index_pt1][1]),(array_boxes_detected[index_pt1][2],array_boxes_detected[index_pt1][3]),(0,0,255),2)
              cv2.rectangle(frame,(array_boxes_detected[index_pt2][0],array_boxes_detected[index_pt2][1]),(array_boxes_detected[index_pt2][2],array_boxes_detected[index_pt2][3]),(0,0,255),2)


  # Write the both outputs video to a local folders
  if output_video_1 is None and output_video_2 is None:
    fourcc1 = cv2.VideoWriter_fourcc(*"MJPG")
    output_video_1 = cv2.VideoWriter("out_video/video1.avi", fourcc1, 25,(frame.shape[1], frame.shape[0]), True)
    fourcc2 = cv2.VideoWriter_fourcc(*"MJPG")
    output_video_2 = cv2.VideoWriter("out_video/bird_view.avi", fourcc2, 25,(bird_view_img.shape[1], bird_view_img.shape[0]), True)
  elif output_video_1 is not None and output_video_2 is not None:
    output_video_1.write(frame)
    output_video_2.write(bird_view_img)
  if cnt==200:
    break
